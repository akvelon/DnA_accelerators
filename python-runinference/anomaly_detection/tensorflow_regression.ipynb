{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Install and import dependencies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import hdbscan\n",
    "import apache_beam as beam\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "\n",
    "from apache_beam import dataframe\n",
    "from apache_beam.ml.inference.base import RunInference, PredictionResult\n",
    "from apache_beam.ml.inference.sklearn_inference import SklearnModelHandlerNumpy\n",
    "from apache_beam.ml.inference.sklearn_inference import ModelFileType\n",
    "from apache_beam.runners.interactive.interactive_runner import InteractiveRunner\n",
    "from apache_beam.ml.inference.pytorch_inference import PytorchModelHandlerTensor, _convert_to_result\n",
    "from apache_beam.ml.inference.base import KeyedModelHandler\n",
    "from autoembedder import Autoembedder\n",
    "import apache_beam.runners.interactive.interactive_beam as ib"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data = pd.read_csv('dtc_opportunity.csv')\n",
    "train, test = train_test_split(data, test_size=0.33)\n",
    "train.to_csv('dtc_opportunity_train.csv', index=False)\n",
    "test.to_csv('dtc_opportunity_test.csv', index=False)\n",
    "\n",
    "# pretrained_embedder_path = f\"gs://{PROJECT_ID}-ml-examples/{MODEL_NAME}/pytorch_model.bin\"\n",
    "# pretrained_anomaly_detection_path = f\"gs://{PROJECT_ID}-ml-examples/{MODEL_NAME}/pytorch_model.bin\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function for one-hot encoding the data. Convert the categorical columns to numerical"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def get_one_hot_encoding(df, col) -> pd.DataFrame:\n",
    "    beam_df_categorical = df[col]\n",
    "    with dataframe.allow_non_parallel_operations():\n",
    "        unique_classes = pd.CategoricalDtype(ib.collect(beam_df_categorical.unique(as_series=True)))\n",
    "    beam_df_categorical = beam_df_categorical.astype(unique_classes).str.get_dummies()\n",
    "    beam_df_categorical = beam_df_categorical.add_prefix(f'{col}___')\n",
    "\n",
    "    return beam_df_categorical"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Custom wrappers for embedding and clustering models\n",
    "These custom handlers are used to determine the way the data processed before the input to RunInference:\n",
    "1. For anomaly detection model\n",
    "2. For PyTorch model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "class CustomSklearnModelHandlerNumpy(SklearnModelHandlerNumpy):\n",
    "    def run_inference(self, batch, model, inference_args=None):\n",
    "        predictions = hdbscan.approximate_predict(model, batch)\n",
    "        return [PredictionResult(x, y) for x, y in zip(batch, predictions[0])]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "class CustomPytorchModelHandlerTensor(PytorchModelHandlerTensor):\n",
    "    def run_inference(self, batch, model, inference_args=None):\n",
    "        with torch.no_grad():\n",
    "            list_of_cont_tensors = []\n",
    "            list_of_cat_tensors = []\n",
    "            for item in batch:\n",
    "                list_of_cont_tensors.append(item['cont_cols'])\n",
    "                list_of_cat_tensors.append(item['cat_cols'])\n",
    "\n",
    "            batched_tensors_cont = torch.stack(list_of_cont_tensors).view(1, -1)\n",
    "            batched_tensors_cat = torch.stack(list_of_cat_tensors).T\n",
    "\n",
    "            model(x_cont=batched_tensors_cont, x_cat=batched_tensors_cat)\n",
    "            predictions = model.code_value\n",
    "            return _convert_to_result(batch, predictions)\n",
    "\n",
    "    def get_num_bytes(self, batch) -> int:\n",
    "        return sum((el[key].element_size() for el in batch for key in el.keys()))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Decode predictions ParDo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "class DecodePrediction(beam.DoFn):\n",
    "    def process(self, element):\n",
    "        uid, prediction = element\n",
    "        cluster = prediction.inference.tolist()\n",
    "        bq_dict = {\"id\": uid, \"cluster\": cluster}\n",
    "        yield bq_dict"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Define model parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "anomaly_detection_model_handler = CustomSklearnModelHandlerNumpy(model_uri='pretrained/anomaly_detection.model',\n",
    "                                                                 model_file_type=ModelFileType.JOBLIB)\n",
    "\n",
    "encoder_handler = CustomPytorchModelHandlerTensor(state_dict_path='pretrained/encoder.pth',\n",
    "                                            model_class=Autoembedder, model_params={'config': parameters, 'num_cont_features': num_cont_features, 'embedding_sizes': list_cat})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Construct and run the pipeline"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "def process_with_pipeline(data_csv, prepare_only_dataframe=False):\n",
    "    with beam.Pipeline(InteractiveRunner()) as p:\n",
    "        input_data = p | \"Read CSV\" >> beam.dataframe.io.read_csv(data_csv)\n",
    "        # For now dropping dates\n",
    "        input_data = input_data.drop(['Billing State/Province',\n",
    "                                      'Created Date',\n",
    "                                      'Close Date',\n",
    "                                      'Account Name',\n",
    "                                      'Product Name',\n",
    "                                      'Opportunity Name'], axis=1)\n",
    "        input_data = input_data.drop('#', axis=1)\n",
    "        input_data = input_data.replace('-', 'HYPHEN', regex=True)\n",
    "        input_data = input_data.replace('/', 'SLASH', regex=True)\n",
    "        input_data = input_data.replace(' ', 'SPACE', regex=True)\n",
    "        input_data = input_data.rename(columns={x: x.replace(' ', '_') for x in input_data.columns})\n",
    "\n",
    "\n",
    "        with dataframe.allow_non_parallel_operations():\n",
    "            input_data = input_data.dropna()\n",
    "\n",
    "        numerical_cols = input_data.select_dtypes(include=np.number).columns.tolist()\n",
    "        bool_cols = input_data.select_dtypes(include='bool').columns.tolist()\n",
    "\n",
    "        categorical_cols = list(set(input_data.columns) - set(numerical_cols) - set(bool_cols))\n",
    "\n",
    "        input_data_numericals = input_data.filter(items=numerical_cols)\n",
    "        input_data_numericals = (input_data_numericals - input_data_numericals.mean())/input_data_numericals.std()\n",
    "\n",
    "        processed_cat_columns = []\n",
    "\n",
    "        for categorical_col in categorical_cols:\n",
    "            input_data_categorical = get_one_hot_encoding(df=input_data, col=categorical_col)\n",
    "            processed_cat_columns.extend(list(input_data_categorical.columns))\n",
    "            input_data_numericals = input_data_numericals.merge(input_data_categorical, left_index=True, right_index=True)\n",
    "\n",
    "        for bool_col in bool_cols:\n",
    "            input_data_numericals = input_data_numericals.merge(input_data[bool_col].astype(int), left_index=True, right_index=True)\n",
    "\n",
    "        if prepare_only_dataframe:\n",
    "            print('Saving only test/train dataframes. Prediction pipeline aborted')\n",
    "            aa = ib.collect(input_data_numericals)\n",
    "            train = aa.sample(frac=0.8, random_state=42)\n",
    "            test = aa.drop(train.index)\n",
    "            train.to_csv('output_train_final.csv', index=False)\n",
    "            test.to_csv('output_test_final.csv', index=False)\n",
    "            return None\n",
    "\n",
    "\n",
    "        input_data_numericals['id'] = input_data_numericals.index\n",
    "        input_data_numericals = beam.dataframe.convert.to_pcollection(input_data_numericals)\n",
    "\n",
    "        keyed_anomalies = (\n",
    "                input_data_numericals\n",
    "                | \"Add id\" >> beam.Map(lambda x: (x.id, {'cont_cols': torch.Tensor([getattr(x, f) for f in numerical_cols]),\n",
    "                                                            'cat_cols': torch.tensor([getattr(x, f) for f in processed_cat_columns+bool_cols])}))\n",
    "                | \"Encode\" >> RunInference(model_handler=KeyedModelHandler(encoder_handler))\n",
    "                | \"Concat features\" >> beam.Map(lambda x: (x[0], x[1].inference.detach().numpy()))\n",
    "                | \"Detect anomaly\" >> RunInference(model_handler=KeyedModelHandler(anomaly_detection_model_handler))\n",
    "                | \"Get predictions\" >> beam.ParDo(DecodePrediction())\n",
    "                | \"Write output\" >> beam.io.textio.WriteToText(file_path_prefix='prediction_output/shard')\n",
    "        )\n",
    "\n",
    "        return keyed_anomalies"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
